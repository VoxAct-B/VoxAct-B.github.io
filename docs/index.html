<!DOCTYPE html>
<html>

<head>
  <meta charset="utf-8">
  <meta name="description"
    content="VoxAct-B: Vision-Language Models and Voxel Representations for Bimanual Manipulation">
  <meta name="keywords" content="Imitation from Observation, Learning from Demonstration">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>VoxAct-B: Vision-Language Models and Voxel Representations for Bimanual Manipulation</title>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/favicon.ico">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>

<body>

  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">VoxAct-B: Vision-Language Models and Voxel Representations
              for Bimanual Manipulation</h1>
            <div class="is-size-5 publication-authors">
              <span class="author-block"><b>Anonymous Author(s)</b></span>
            </div>

            <div class="is-size-5 publication-authors">
              <span class="author-block">Affiliation</span>
            </div>

            <div class="column has-text-centered">
              <div class="publication-links">
                <!-- PDF Link. -->
                <span class="link-block">
                  <a href="" target="_blank" class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                    </span>
                    <span>Paper</span>
                  </a>
                </span>
                <span class="link-block">
                  <a href="" target="_blank" class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="ai ai-arxiv"></i>
                    </span>
                    <span>arXiv (coming soon)</span>
                  </a>
                </span>
                <!-- Video Link. -->
                <!-- <span class="link-block">
                <a href="https://www.youtube.com/watch?v=MrKrnHhk8IA"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-youtube"></i>
                  </span>
                  <span>Video</span>
                </a>
              </span> -->
                <!-- Code Link. -->
                <span class="link-block">
                  <a href="" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code (coming soon)</span>
                  </a>
                </span>
                <!-- Dataset Link. -->
                <!-- <span class="link-block">
                <a href="https://github.com/google/nerfies/releases/tag/0.1"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="far fa-images"></i>
                  </span>
                  <span>Data</span>
                  </a>
              </span> -->
              </div>
            </div>
          </div>
        </div>
      </div>
    </div>
  </section>

  <section class="hero teaser">
    <div class="container is-max-desktop">
      <div class="hero-body">
        <!-- Video in the future -->
        <img id="teaser" src="./static/images/figure1_v04.png" alt="Figure 1">
        <!-- <video id="teaser" autoplay muted loop playsinline height="100%">
          <source src="./static/videos/hero.mp4" type="video/mp4">
        </video> -->
        <!-- <h2 class="subtitle has-text-centered">
        <span class="dnerf">Nerfies</span> turns selfie videos from your phone into
        free-viewpoint
        portraits.
      </h2> -->
      </div>
    </div>
  </section>


  <!-- <section class="hero is-light is-small">
  <div class="hero-body">
    <div class="container">
      <div id="results-carousel" class="carousel results-carousel">
        <div class="item item-steve">
          <video poster="" id="steve" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/steve.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-chair-tp">
          <video poster="" id="chair-tp" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/chair-tp.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-shiba">
          <video poster="" id="shiba" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/shiba.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-fullbody">
          <video poster="" id="fullbody" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/fullbody.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-blueshirt">
          <video poster="" id="blueshirt" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/blueshirt.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-mask">
          <video poster="" id="mask" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/mask.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-coffee">
          <video poster="" id="coffee" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/coffee.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-toby">
          <video poster="" id="toby" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/toby2.mp4"
                    type="video/mp4">
          </video>
        </div>
      </div>
    </div>
  </div>
</section> -->


  <section class="section">
    <div class="container is-max-desktop">
      <!-- Abstract. -->
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3">Abstract</h2>
          <div class="content has-text-justified">
            <p>
              Bimanual manipulation is critical to many robotics applications.
              In contrast to single-arm manipulation, bimanual manipulation tasks is challenging due to
              higher-dimensional action spaces.
              Prior works leverage large amounts of data and primitive actions to address this problem.
              However, they do not scale up and generalize well to multiple tasks.
              To this end, we propose leveraging large language models (LLMs) to enable a more generalized policy and
              learning from voxels for efficient policy learning.
              We use LLMs to navigate free space and utilize voxel-based behavior cloning for contact-rich manipulation.
              We evaluate the proposed method on an open jar task built in RLBench and compare it against three
              baselines and ablations.
              The results show that the proposed method outperforms the baselines by a significant margin.
              Code, data, prompts, and videos will be available at
            </p>

          </div>
        </div>
      </div>
      <!--/ Abstract. -->

      <!-- Paper video. -->
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3">Video</h2>
          <div class="publication-video">
            <p style="font-size: 36pt;"><b>We need a video here</b></p>
            <!-- <iframe src="https://www.youtube.com/embed/y7IM83ASzvQ?si=munU7x2ALzU0fzhM" frameborder="0"
              allow="autoplay; encrypted-media" allowfullscreen></iframe> -->
          </div>
        </div>
      </div>
      <!--/ Paper video. -->

      <!-- Overview. -->
      <br />
      <div class="columns is-centered">
        <div class="column is-full-width has-text-justified">
          <h2 class="title is-3">Overview</h2>
          <br />
          <img src="./static/images/figure2_v2.png" alt="MAIL schematic">
          <p>
            <center>Overview of VoxAct-B. The inputs consist of RGB-D images, a language goal, and proprioception
              data. First, the RGB-D images and language goal are fed into VoxPoser to generate code that allows the
              robot arms to navigate through free space and reach a region close to the object of interest. Next, a
              zoomed-in,
              upsampled voxel grid of the target object, language goal, and proprioception data are fed into PerAct to
              perform
              contact-rich manipulation.</center>
          </p>
          <br />
          <p>
            Bimanual manipulation is essential for robotics tasks, such as when objects are too large to be
            controlled by one gripper, or when manipulation requires one arm to stabilize the object of interest
            to make it simpler for the other arm. Designing a bimanual manipulation policy is challenging due
            to the high dimensionality of the action space. To tackle this problem, some methods train
            policies on large datasets, and some exploit primitive actions. However, they tend
            to struggle with generalization to multiple tasks.
          </p>
          <br />
          <p>
            In this work, we propose to explore how to leverage voxel representations for bimanual manipu-
            lation. Voxel representations, when coupled with discretized action spaces, can increase sample
            efficiency and generalization by introducing spatial equivariance into a learned system, where trans-
            formations of the input lead to corresponding transformations of the outputs. Specifically, we
            focus on asymmetric bimanual manipulation. Asymmetry refers
            to how the left and right arms have different functions. In our tasks, for example, one acts as an
            assistive arm to support the other arm's action. Asymmetric tasks are common in household and
            industrial settings, such as cutting food, opening bottles, and packaging boxes. However, these tasks
            typically require two-hand coordination and high-precision, fine-grained manipulation, which are
            very challenging for current robotic manipulation systems.
          </p>
          <br />
          <p>
            We propose VoxAct-B, a method that leverages the strong reasoning capability and
            generalization of Large Language Models (LLMs) and Vision Language Models (VLM) for free-
            space traversal, along with downstream contact-rich manipulation using behavioral cloning. We
            extend the RLBench benchmark to support bimanual manipulation, and show that VoxAct-B
            reduces the search space and exploration burden of behavioral cloning, allowing it to better learn
            contact-rich motions from demonstrations as compared to baselines and ablations. We demonstrate
            VoxAct-B on multiple simulation tasks and in real using a dual UR5 system.
          </p>
        </div>
      </div>
      <!--/ Overview. -->

      <!-- Results. -->
      <br />
      <div class="columns is-centered">
        <div class="column is-full-width">
          <h2 class="title is-3">Results</h2>


          <h3 class="title is-4">Open Jar</h3>
          <div class="content has-text-justified">
            <p>
              A jar with a screw-on lid is randomly spawned and scaled from 90% to 100% of
              the original size within the robot's workspace. The jar's color is also varied to introduce visual
              diversity. The robot must first grasp the jar with one hand and then use the other hand to unscrew
              the lid in an anti-clockwise direction until it is completely removed.
            </p>
          </div>
          <video controls muted height="100%">
            <source src="./static/videos/open_jar.mp4" type="video/mp4">
          </video>
          <br />

          <br />
          <h3 class="title is-4">Open Drawer</h3>
          <div class="content has-text-justified">
            <p>
              The robot needs to hold the drawer with one hand and then open the bottom drawer
              with the other. The drawer is randomly scaled from 90% to 100% of its original size. Additionally,
              the rotation of the drawer is randomized between -π/8 and π/8 to add variability and challenge to
              the task.
            </p>
          </div>
          <video controls muted height="100%">
            <source src="./static/videos/open_drawer.mp4" type="video/mp4">
          </video>
          <br />

          <br />
          <h3 class="title is-4">Put Item in Drawer</h3>
          <div class="content has-text-justified">
            <p>
              An item is on top of a drawer. Both the drawer and the item are randomly
              scaled by the same value within the range of 0.9 to 1. The drawer is also randomly rotated. The
              robot needs to open the top drawer with one hand and place the item inside it with the other hand.
            </p>
          </div>
          <video controls muted height="100%">
            <source src="./static/videos/put_item_in_drawer.mp4" type="video/mp4">
          </video>
          <br />



          <br />
          <h3 class="title is-4">Performance Comparisons</h3>
          <div class="content has-text-justified">
            <p>
              For each training run, we used the best model in each seed's training run, and evaluated using 25
              rollouts across 3 seeds.
              The results show that both ACT and Diffusion Policy are constrained by the training data.
              Ours outperforms the baselines by a significant margin.
              Specifically, the improvements from Two PerActs to
              ours come from using VoxPoser for navigation through free space and using zoomed-in, upsampled voxel grids
              for PerAct.
            </p>
          </div>
          <img class="center" src="./static/images/table_success_rate.png" alt="Performance Comparisons">
        </div>
      </div>
    </div>
  </section>

  <!-- <section class="section" id="Acknowledgements">
    <div class="container is-max-desktop content">
      <h2 class="title">Acknowledgements</h2>
    </div>
  </section> -->

  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">BibTeX</h2>
      <pre><code>
      </code></pre>
    </div>
  </section>


  <footer class="footer">
    <div class="container">
      <div class="columns is-centered">
        <div class="column is-8">
          <div class="content">
            <p>
              This website template is borrowed from <a
                href="https://github.com/nerfies/nerfies.github.io">nerfies.github.io</a>.
            </p>
          </div>
        </div>
      </div>
    </div>
  </footer>

</body>

</html>